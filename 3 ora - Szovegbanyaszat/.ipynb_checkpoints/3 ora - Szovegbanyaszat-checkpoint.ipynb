{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Szövegbányászat (Natural Language Processing, NLP)\n","A szociális médiában (is) óriási mennyiségű adat érhető el strukúrálatlan, például szöveges formában. Ezen adatok automatikus feldolgozását nevezzük szövegbányászatnak vagy nyelvtechnológiának (angolul Natural Language Processing).\n","\n","Az emberi nyelvek gyönyörűek. Ugyanazt száz féle képpen ki tudjuk fejezni és egy szó vagy kifejezés sok mindent tud jelenteni más mondatkörnyezetben. Az emberek által, emberi olvasásra írt szövegek gépi *értelmezése* máig megoldatlan probléma. Azonban a szövegek feldolgozásában rengeteget fejlődött a technológia az elmúlt 20 évben.\n","\n","\n","### További anyagok:\n","*   D. Jurafsky: Intorduction to NLP [youtube](https://www.youtube.com/watch?v=oWsMIW-5xUc)\n","*   HuSpaCy magyar nyelvű szövegfeldolgozó rendszer [demo oldala](https://huggingface.co/spaces/huspacy/demo)\n","\n","\n"],"metadata":{"id":"EVEjRo5BFnSG"}},{"cell_type":"markdown","source":["# Tokenizálás\n","Dokumentumok hosszú string reprezentációjával nem lehet adatelemzésben dolgozni, szükség van azok előfeldolgozására, olyan egységekre bontásra, ami már kezelhető: mondatok és szavak lesznek az egység.\n","\n","Az [NLTK](https://www.nltk.org/) és a [spaCy](https://spacy.io/) a leggyakrabban használt python csomagok szövegfeldolgozásban. Kettejük egy összehasonlítása [itt](https://www.activestate.com/blog/natural-language-processing-nltk-vs-spacy/)\n","\n"],"metadata":{"id":"1zYHMBhlGCB1"}},{"cell_type":"markdown","source":["### Adatok betöltése – Kaggle-SMS-Spam-Collection-Dataset\n","\n","A Kaggle SMS Spam Collection Dataset egy népszerű nyilvános adathalmaz, amelyet SMS-ek spam detekciójára használnak.\n","- Tisztított, valódi SMS-eket tartalmaz, amelyeket két kategóriába soroltak:\n","  - \"ham\" (normál üzenet)\n","  - \"spam\" (kéretlen reklám, csalás, stb.)\n","\n"," Eredeti forrás: A dataset a UCI Machine Learning Repository-ból származik, amelyet kutatási célokra gyűjtöttek össze.\n","\n","Felhasználási területek:\n","- Spam detekció gépi tanulással\n","- Szövegbányászat (NLP)\n","- Véleménydetekció és osztályozás\n","\n"],"metadata":{"id":"i-iv8BclGGPk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ONO1RkjFjc7"},"outputs":[],"source":["import pandas as pd\n","\n","# SMS Spam Dataset betöltése\n","url = \"https://github.com/mohitgupta-1O1/Kaggle-SMS-Spam-Collection-Dataset-/raw/refs/heads/master/spam.csv\"\n","\n","# CSV fájl betöltése pandas DataFrame-be\n","# A 'latin1' kódolást használjuk, és figyelmen kívül hagyjuk a hibás sorokat\n","df = pd.read_csv(url, sep=\",\", header=0, encoding='latin1', on_bad_lines='skip')\n","\n","# Csak a két szükséges oszlopot tartjuk meg ('label' és 'message')\n","df = df.iloc[:, :2].rename(columns={'v1': 'label', 'v2': 'message'})\n","\n","# Az első dokumentum szövege\n","print(\"Első dokumentum szövege:\\n\")\n","print(df.message[0])\n","\n"]},{"cell_type":"markdown","source":["### Tokenizálás – Szavakra és mondatokra bontás\n","- A tokenizálás az első lépés az NLP-ben, ahol a szöveget kisebb egységekre (tokenekre) bontjuk.\n","- Használhatunk nltk, spaCy, vagy scikit-learn megoldásokat."],"metadata":{"id":"MLEzb6LiGTrp"}},{"cell_type":"markdown","source":["### Tokenizálás NLTK segítségével\n","- A szöveget mondatokra (sent_tokenize) és szavakra (word_tokenize) bontjuk."],"metadata":{"id":"ukppw3pXGb1N"}},{"cell_type":"code","source":["pip install --upgrade nltk"],"metadata":{"id":"OAJc91IgHqm_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","import os\n","nltk.data.path.append(os.path.expanduser('~/nltk_data'))\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","# Első dokumentum kiválasztása\n","text = df.message[0]"],"metadata":{"id":"OxZ_vgJ9GNVh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mondatokra bontás\n","sentences = sent_tokenize(text)\n","print(\"\\nMondatok száma:\", len(sentences))\n","display(\"Első mondat:\", sentences[0])"],"metadata":{"id":"jSLts3I385wd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Szavakra bontás\n","words = word_tokenize(text)\n","print(\"\\nTokenek száma:\", len(words))\n","print(\"Első 10 szó:\", words[:10])"],"metadata":{"id":"ChVazgc_8541"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- Mondatokra bontás (sent_tokenize) -> Megszámoljuk, hány mondat van.\n","- Szavakra bontás (word_tokenize) -> Megnézzük az első 10 szót."],"metadata":{"id":"wQ7MVtlUH907"}},{"cell_type":"markdown","source":["###Tokenizálás SpaCy segítségével\n","\n","A SpaCy egy erősebb NLP könyvtár, amely gyorsabb és több funkcióval rendelkezik."],"metadata":{"id":"G8EXq-htIHyd"}},{"cell_type":"code","source":["import spacy\n","\n","# Angol nyelvű NLP modell betöltése\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# SpaCy tokenizálás\n","doc = nlp(text)\n","\n","# Mondatok kiíratása\n","sentences_spacy = list(doc.sents)\n","print(\"\\nSpaCy mondatok száma:\", len(sentences_spacy))\n","print(\"Első mondat:\", sentences_spacy[0])\n","\n","# Szavak kiíratása\n","tokens_spacy = [token.text for token in doc]\n","print(\"\\nSpaCy első 10 szó:\", tokens_spacy[:10])\n"],"metadata":{"id":"tc9fsrTIGgvR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Miért jó a SpaCy?\n","\n","- Gyorsabb, mint NLTK, mert optimalizált C-kódban fut.\n","- Nemcsak szavakat, hanem entitásokat, szófajokat is felismer."],"metadata":{"id":"U1VhKM7tIUrL"}},{"cell_type":"markdown","source":["### Tokenizálás Scikit-learn segítségével\n","\n","A CountVectorizer automatikusan tokenizál és előkészíti az adatokat későbbi modellekhez."],"metadata":{"id":"NXt8NpwRIZJF"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(df.message)\n","\n","print(\"\\nTokenek száma az egész dataset-ben:\", len(vectorizer.get_feature_names_out()))\n","print(\"Első 10 token:\", vectorizer.get_feature_names_out()[:10])\n"],"metadata":{"id":"wKjW98cjGsOR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- A CountVectorizer egy BoW (Bag-of-Words) modellt készít.\n","- Minden egyes egyedi szót megtalálja és megszámolja az előfordulását."],"metadata":{"id":"LDBlISsXIiM5"}},{"cell_type":"markdown","source":["Mit jelent a fura token lista?\n","\n","- A vektorizáló valójában az SMS-ek tartalmát próbálta átalakítani tokenekké, de mivel az üzenetekben sok szám is szerepel, a következő történhetett:\n","['00' '000' '000pes' '008704050406' '0089' '0121' '01223585236']\n","\n","Megoldás – Számok kizárása\n","- Ha nem szeretnéd, hogy számokat is tokenizáljon, akkor a CountVectorizer() beállításaival ezt szabályozhatod:"],"metadata":{"id":"LPNQ2qFN7DB9"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(token_pattern=r'\\b[a-zA-Z]+\\b')\n","X = vectorizer.fit_transform(df.message)\n","\n","print(\"\\nTokenek száma az egész dataset-ben:\", len(vectorizer.get_feature_names_out()))\n","print(\"Első 10 token:\", vectorizer.get_feature_names_out()[:10])\n"],"metadata":{"id":"Eo0-QoPY7GU6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tokenizálás vizualizálása – Szavak gyakorisági eloszlása\n","\n","Most készítsünk egy hisztogramot a leggyakoribb szavakról."],"metadata":{"id":"AS-vDE6JInGR"}},{"cell_type":"code","source":["from collections import Counter\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Tokenek gyakorisági elemzése\n","word_counts = Counter(words)\n","common_words = word_counts.most_common(15)\n","\n","# Adatok elkülönítése a vizualizációhoz\n","words, counts = zip(*common_words)\n","\n","plt.figure(figsize=(10, 5))\n","sns.barplot(x=list(words), y=list(counts), palette=\"coolwarm\")\n","plt.xticks(rotation=45)\n","plt.xlabel(\"Szavak\")\n","plt.ylabel(\"Előfordulás\")\n","plt.title(\"Leggyakoribb 15 szó\")\n","plt.show()\n"],"metadata":{"id":"J2nwJ8XaIdhC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit látunk?\n","\n","- A legtöbbször előforduló szavak gyakoriságát a datasetben.\n","- A későbbiekben stopwords szűrést kell alkalmazni, hogy a \"the\", \"and\" stb. ne legyen benne.\n","\n","\n","A tokenizálás az NLP első lépése, amely segít a szöveget strukturált adatokká alakítani.\n","A következő lépés: Stopwords eltávolítása és Szófaji elemzés (POS tagging).\n"],"metadata":{"id":"RoauWgXRIu3O"}},{"cell_type":"markdown","source":["### Miért nem egyértelmű a tokenizálás? Miért léteznek különböző algoritmusok?\n","\n","- Azért mert különböző nyelveken másképp lehetnek a szóhatárok (magyar \"-e\"), például rövidítést jelentő pont a token része (\"U.S.A.\" vagy \"kft.\"), míg mondatvégi a írásjel nem.\n","- A szöveg típusa is megkövetelhet különböző tokenizálókat, pl. szociális médiában az emotikonok és URLeket egyben kell tartni de a camelcase szavakat (pl. JoMunkahozIdoKell) tokenizáljuk, kémiai szövegekben a kötőjel egy molekula nevében nem szóhatár, stb."],"metadata":{"id":"esn-OB16JD-H"}},{"cell_type":"markdown","source":["### Stopwords eltávolítása\n","\n","- A stopwords azok a szavak, amelyek gyakran előfordulnak, de általában nem hordoznak fontos információt (pl. \"the\", \"is\", \"in\", \"on\", \"and\").\n","- Stopwords eltávolítása segít csökkenteni az adatdimenziót és jobb modelleket készíteni."],"metadata":{"id":"sbafzlqDJh6R"}},{"cell_type":"markdown","source":["### Stopwords eltávolítása NLTK segítségével\n","- NLTK beépített angol stopwords listáját használjuk."],"metadata":{"id":"sanHy2QkQoJl"}},{"cell_type":"code","source":["import nltk\n","nltk.download(\"stopwords\")\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Stopwords lista betöltése\n","stop_words = set(stopwords.words(\"english\"))\n","\n","# Tokenizálás (szavakra bontás)\n","words = word_tokenize(text)\n","\n","# Stopwords eltávolítása\n","filtered_words = [word for word in words if word.lower() not in stop_words]\n","\n","print(\"\\nEredeti szólista:\", words[:20])\n","print(\"\\nStopwords eltávolítása után:\", filtered_words[:20])\n"],"metadata":{"id":"UIx6JNoPQhLR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- Tokenizáljuk a szöveget (word_tokenize()).\n","- Eltávolítjuk a gyakori angol stopwords szavakat (pl. \"the\", \"is\", \"on\", \"and\").\n","- Az eredmény egy tisztább szöveg, amely kevesebb felesleges szót tartalmaz."],"metadata":{"id":"no5Yp5_fQxCu"}},{"cell_type":"markdown","source":["### Stopwords eltávolítása SpaCy segítségével\n","- SpaCy beépített stopwords listáját használjuk, amely gyorsabb és pontosabb lehet."],"metadata":{"id":"3m7gxoaZQ6Sd"}},{"cell_type":"code","source":["import spacy\n","\n","# Angol nyelvű NLP modell betöltése\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# SpaCy tokenizálás és stopwords eltávolítása\n","doc = nlp(text)\n","filtered_words_spacy = [token.text for token in doc if not token.is_stop]\n","\n","print(\"\\nStopwords eltávolítása után SpaCy-val:\", filtered_words_spacy[:20])\n"],"metadata":{"id":"-2Zfe9E3QskA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- Tokenizáljuk a szöveget SpaCy-vel.\n","- Automatikusan eltávolítjuk a stopwords szavakat.\n","- Az eredmény egy tisztább szólista."],"metadata":{"id":"YYiv_x3_RALt"}},{"cell_type":"markdown","source":["### Szófaji elemzés (POS Tagging)\n","\n","---\n","\n","\n","- A szófaji elemzés (Part-Of-Speech Tagging, POS Tagging) meghatározza, hogy egy szó milyen szófaji kategóriába tartozik (pl. főnév, ige, melléknév).\n","- Ez segít megérteni a mondat szerkezetét és a fontosabb szavakat kiemelni."],"metadata":{"id":"xLHiZZObREuh"}},{"cell_type":"markdown","source":["### POS Tagging NLTK segítségével\n","- NLTK beépített POS címkézőjét használjuk."],"metadata":{"id":"HPimyXkGRLys"}},{"cell_type":"code","source":["import nltk\n","import os\n","\n","nltk.data.path.append(os.path.expanduser('~/nltk_data')) #vagy a megfelelő könyvtár\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('averaged_perceptron_tagger_eng')"],"metadata":{"id":"10jmXqYZQ9-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Szófaji címkézés NLTK-val\n","pos_tags = nltk.pos_tag(filtered_words)\n","print(\"\\nSzófaji címkék (NLTK):\", pos_tags[:10])"],"metadata":{"id":"DEpuQ_3F-IoL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- okenizáljuk a szöveget és eltávolítjuk a stopwords-t.\n","- POS címkézéssel megjelöljük a szavakat, hogy főnév (NN), ige (VB), melléknév (JJ) stb.\n","\n","- bővebben a röviditésekről: https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/"],"metadata":{"id":"_ReCdw76Rzul"}},{"cell_type":"markdown","source":["### POS Tagging SpaCy segítségével\n","- SpaCy beépített POS címkézője gyorsabb és pontosabb lehet."],"metadata":{"id":"PQJ-FfjkSklm"}},{"cell_type":"code","source":["# SpaCy POS tagging\n","pos_tags_spacy = [(token.text, token.pos_) for token in doc]\n","\n","print(\"\\nSzófaji címkék (SpaCy):\", pos_tags_spacy[:10])\n"],"metadata":{"id":"ykhLp3TLRPHh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- SpaCy automatikusan POS címkézi a szöveget.\n","- Az eredmény ember-barátabb és könnyebben értelmezhető."],"metadata":{"id":"xYzdwQqySu-V"}},{"cell_type":"markdown","source":["### POS Tagging vizualizálása – Szófaji eloszlás\n","- Készítsünk egy oszlopdiagramot, amely megmutatja a szavak szófaji eloszlását!"],"metadata":{"id":"urTxxS82S0zp"}},{"cell_type":"code","source":["from collections import Counter\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# POS címkék gyakorisági eloszlása\n","pos_counts = Counter([pos for word, pos in pos_tags_spacy])\n","\n","# Adatok vizualizálása\n","plt.figure(figsize=(10, 5))\n","sns.barplot(x=list(pos_counts.keys()), y=list(pos_counts.values()), palette=\"coolwarm\")\n","plt.xlabel(\"Szófaj\")\n","plt.ylabel(\"Előfordulás\")\n","plt.title(\"Szófaji címkék eloszlása\")\n","plt.show()\n"],"metadata":{"id":"4VNfDRwsSqlx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit látunk?\n","\n","- Mely szófajok dominálnak a szövegben.\n","- Ha például túl sok ige van, az egy akcióközpontú szöveget jelenthet."],"metadata":{"id":"f3NP08BMS6qp"}},{"cell_type":"markdown","source":["### Stemming és a Lemmatizálás\n","\n","- Még egy fontos előfeldolgozási lépés a szótövesítés (stemming, lemmatizáció)\n","- A cél, hogy a szavak különböző ragozott alakjait össze tudjuk vonni (pl. általában nem érdemes külön kezelni az 'asztalaitokra' és 'asztal' szóalakokat).\n","- A lemmatizáció az igazi nyelvtani értelemben vett szótő meghatározását jelenti. Ez nagyon bonyolult feladat tud lenni bizonyos nyelveken, pl. a magyarban ahol tőhangváltás is van (a 'madarak' szó szótöve a 'madár').\n","- Sokszor elég a szótőnek egy \"közelítése\", azaz egyszerű szabályokkal lecseréljük a szóalak végi karaktereket más karakterre.\n","- Ezt a közelítő (butább, de sokkal egyszerűbb és gyorsabb) hívjuk stemmelésnek (részletesen)"],"metadata":{"id":"0_n9twEhTZBt"}},{"cell_type":"markdown","source":["- Stemming (származtatás): A szó tövére vágása szabály alapú módszerekkel (pl. \"running\"-> \"run\").\n","- Lemmatizálás: A szó szótári alakjára történő visszavezetése (pl. \"better\" -> \"good\").\n","- A lemmatizálás pontosabb, mert a szó jelentését is figyelembe veszi."],"metadata":{"id":"cgffrPvOTQ4K"}},{"cell_type":"markdown","source":["### Stemming (Származtatás) NLTK segítségével\n","- NLTK két beépített stemmert tartalmaz: PorterStemmer és LancasterStemmer"],"metadata":{"id":"d0VRqSwWUCF6"}},{"cell_type":"code","source":["import nltk\n","from nltk.stem import PorterStemmer, LancasterStemmer\n","from nltk.tokenize import word_tokenize\n","\n","# Tokenizálás\n","words = word_tokenize(text)\n","\n","# Két különböző stemmer tesztelése\n","porter = PorterStemmer()\n","lancaster = LancasterStemmer()\n","\n","# Példa stemming eredményekre\n","stemmed_porter = [porter.stem(word) for word in words[:20]]\n","stemmed_lancaster = [lancaster.stem(word) for word in words[:20]]\n","\n","print(\"\\nEredeti szavak:\", words[:20])\n","print(\"\\nPorter Stemming:\", stemmed_porter)\n","print(\"\\nLancaster Stemming:\", stemmed_lancaster)\n"],"metadata":{"id":"aMvoLQCPS3qQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- Tokenizáljuk a szöveget (word_tokenize()).\n","- Kétféle stemming algoritmust tesztelünk:\n","  - Porter Stemmer: Precízebb, kevésbé agresszív.\n","  - Lancaster Stemmer: Agresszívebb, erősebben rövidít.\n","- Eredmény: A szavakat tövére csonkoljuk.\n","\n","Mikor használjuk a stemminget?\n","- Amikor gyors és egyszerű megoldásra van szükség, például keresőmotorokhoz"],"metadata":{"id":"eyzVhqLjUM-C"}},{"cell_type":"markdown","source":["### Lemmatizálás (Lemmatization) NLTK segítségével\n","- NLTK a WordNetLemmatizer függvényt használja, amely pontosabb, mint a stemming."],"metadata":{"id":"hYHYbnaaUaqt"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","\n","# Lemmatizer inicializálása\n","lemmatizer = WordNetLemmatizer()\n","\n","# Példa lemmatizált szavakra\n","lemmatized_words = [lemmatizer.lemmatize(word) for word in words[:20]]\n","\n","print(\"\\nEredeti szavak:\", words[:20])\n","print(\"\\nLemmatizált szavak:\", lemmatized_words)\n"],"metadata":{"id":"s5QlXjosUFqR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- A WordNetLemmatizer visszaadja a szavak szótári alakját.\n","- Pl. \"running\" -> \"run\", \"better\" -> \"better\" (mert nincs szófaji megadva, ezt pontosíthatjuk).\n","\n","Mikor jobb a lemmatizálás?\n","\n","- Ha pontos eredményre van szükség (például szentiment elemzésnél)."],"metadata":{"id":"4wP64kX6UjzC"}},{"cell_type":"markdown","source":["### Lemmatizálás SpaCy segítségével\n","- A SpaCy lemmatizálója fejlettebb, és automatikusan felismeri a szófajokat is."],"metadata":{"id":"5CbX7m1DUrvi"}},{"cell_type":"code","source":["import spacy\n","\n","# SpaCy NLP modell betöltése\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Tokenizálás és lemmatizálás\n","doc = nlp(text)\n","lemmatized_spacy = [token.lemma_ for token in doc][:20]\n","\n","print(\"\\nSpaCy lemmatizált szavak:\", lemmatized_spacy)\n"],"metadata":{"id":"-iJC2hCMUdwA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- Tokenizáljuk a szöveget és automatikusan lemmatizáljuk.\n","- SpaCy pontosabb, mert a szavak szófaját is figyelembe veszi."],"metadata":{"id":"1eciMwpkUyXN"}},{"cell_type":"markdown","source":["### Stemming és Lemmatizálás vizualizálása\n","\n","Vizsgáljuk meg a különbséget egy oszlopdiagram segítségével!"],"metadata":{"id":"gWNH2qi2U1YH"}},{"cell_type":"code","source":["import spacy\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from nltk.stem import PorterStemmer\n","\n","# SpaCy NLP modell betöltése\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Stemming inicializálása NLTK-val\n","porter = PorterStemmer()\n","\n","# Teszt szavak listája\n","words = [\"running\", \"better\", \"flies\", \"talking\", \"ate\", \"mice\", \"swimming\", \"driving\", \"playing\", \"children\"]\n","\n","# Stemming és Lemmatizálás alkalmazása\n","stemmed = [porter.stem(word) for word in words]  # NLTK PorterStemmer\n","lemmatized = [nlp(word)[0].lemma_ for word in words]  # SpaCy lemmatizáció\n","\n","# Adatok DataFrame-be rendezése\n","df_viz = pd.DataFrame({\"Word\": words, \"Stemming\": stemmed, \"Lemmatization\": lemmatized})\n","\n","# Vizualizáció\n","plt.figure(figsize=(10, 5))\n","\n","# Stemming (kék oszlopok)\n","sns.barplot(x=df_viz[\"Word\"], y=[len(word) for word in df_viz[\"Stemming\"]], color=\"blue\", label=\"Stemming\")\n","\n","# Lemmatization (piros oszlopok)\n","sns.barplot(x=df_viz[\"Word\"], y=[len(word) for word in df_viz[\"Lemmatization\"]], color=\"red\", label=\"Lemmatization\", alpha=0.7)\n","\n","# Diagram beállítások\n","plt.xlabel(\"Szavak\")  # X tengely címke\n","plt.ylabel(\"Szó hosszúsága\")  # Y tengely címke\n","plt.legend()  # Jelmagyarázat megjelenítése\n","plt.title(\"Stemming vs. Lemmatizálás összehasonlítása\")  # Diagram címe\n","\n","# Diagram megjelenítése\n","plt.show()\n","\n"],"metadata":{"id":"-ekfeGt1Uumx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit látunk?\n","\n","- A stemming rövidebb szavakat eredményez, de néha pontatlan.\n","- A lemmatizálás hosszabb, de pontosabb, mert figyelembe veszi a szófajt is."],"metadata":{"id":"jfNegqIrVCW2"}},{"cell_type":"markdown","source":["### N-gramok\n","\n","\n","Mi az N-gram és miért fontos?\n","- Az N-gram egy szavakból álló egymást követő egység a szövegben.\n","- Példa:\n","  - Unigram (1-gram): [\"I\", \"love\", \"machine\", \"learning\"]\n","  - Bigram (2-gram): [(\"I\", \"love\"), (\"love\", \"machine\"), (\"machine\", \"learning\")]\n","  - Trigram (3-gram): [(\"I\", \"love\", \"machine\"), (\"love\", \"machine\", \"learning\")]\n","\n","Nagyobb N-gramok jobban megőrzik a szöveg jelentését, de több számítási erőforrást igényelnek.\n","\n"],"metadata":{"id":"hmcxGA_4V_eR"}},{"cell_type":"markdown","source":["### N-gramok generálása NLTK segítségével\n","- Készítsünk Unigramokat, Bigramokat és Trigramokat egy szövegen belül."],"metadata":{"id":"4_Ti2Su7WRCu"}},{"cell_type":"markdown","source":["### SMS Spam Dataset betöltése és előnézet\n","- Először töltsük be az adatokat és nézzük meg az első pár sort."],"metadata":{"id":"C1MEM7ESXOLL"}},{"cell_type":"code","source":["import pandas as pd\n","\n","url = \"https://github.com/mohitgupta-1O1/Kaggle-SMS-Spam-Collection-Dataset-/raw/refs/heads/master/spam.csv\"\n","df = pd.read_csv(url, sep=\",\", header=0,  encoding='latin1', on_bad_lines='skip').iloc[:,:2].rename(columns={'v1':'label','v2':'message'})\n","\n","print(df.head())"],"metadata":{"id":"MreEgKjMWqV2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- Letöltjük az SMS Spam Datasetet\n","- A label oszlop tartalmazza az üzenet típusát (ham = normál, spam = reklám)\n","- A message oszlop az SMS-ek tartalmát"],"metadata":{"id":"FCR2dB34X2BH"}},{"cell_type":"code","source":["print(df[\"message\"].value_counts())  # Osztályok eloszlása\n","print(\"Adathalmaz mérete:\", df.shape)  # Sorok és oszlopok száma\n"],"metadata":{"id":"rzRt3SX-X7K2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### N-gramok generálása\n","- Az N-gramokat az SMS-ekre alkalmazzuk (unigram, bigram, trigram)."],"metadata":{"id":"e1xOthzQYCN2"}},{"cell_type":"code","source":["import nltk\n","from nltk import ngrams\n","from nltk.tokenize import word_tokenize\n","\n","# Első SMS üzenet kiválasztása és tokenizálása\n","sample_text = df['message'][5570]\n","words = word_tokenize(sample_text)\n","\n","# N-gramok generálása\n","unigrams = list(ngrams(words, 1))\n","bigrams = list(ngrams(words, 2))\n","trigrams = list(ngrams(words, 3))\n","\n","print(\"\\nUnigramok:\", unigrams[:10])\n","print(\"\\nBigramok:\", bigrams[:10])\n","print(\"\\nTrigramok:\", trigrams[:10])\n"],"metadata":{"id":"EEdVd1q2U4nB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- Tokenizáljuk az 5570. SMS-t.\n","- Generáljuk az N-gramokat (unigram, bigram, trigram).\n","- Megnézzük az első 10 darabot mindegyikből.\n","\n","Mikor hasznosak az N-gramok?\n","-  Bigramok és trigramok megőrzik a szókapcsolatokat\n","-  Jobb modelleket készíthetünk belőlük, pl. spam detektálásra"],"metadata":{"id":"DlFCVoYiYbPx"}},{"cell_type":"markdown","source":["### WordCloud – Mi ez és miért hasznos?\n","- A WordCloud (szófelhő) egy vizualizációs technika, amely a leggyakoribb szavakat jeleníti meg egy szöveges korpuszból.\n","- A gyakoribb szavak nagyobb méretben jelennek meg, míg a ritkább szavak kisebbek.\n"],"metadata":{"id":"nSVtAq0rnof-"}},{"cell_type":"markdown","source":["Alap WordCloud generálása\n","- Készítsünk egy egyszerű WordCloud-ot egy szöveges adatból!"],"metadata":{"id":"lVpvHFeJnvtz"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","\n","# Minta szöveg\n","text = \"WordCloud is a great way to visualize text data and find the most frequent words in a dataset.\"\n","\n","# WordCloud objektum létrehozása\n","wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n","\n","# Kirajzolás\n","plt.figure(figsize=(10, 5))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis(\"off\")  # Kikapcsoljuk a tengelyeket\n","plt.title(\"Egyszerű WordCloud példa\")\n","plt.show()\n"],"metadata":{"id":"qCMfM2centkb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- Generálunk egy szófelhőt egy minta szövegből.\n","- Fehér háttéren jelenítjük meg a szavakat, a gyakoribb szavak nagyobb méretben látszanak.\n","- Eredmény: Egy szófelhő, amelyben a \"WordCloud\" és \"visualize\" nagyobb, mert ezek a szavak gyakran előfordulnak."],"metadata":{"id":"mTivwgtsn0Yi"}},{"cell_type":"markdown","source":["WordCloud a Twitter véleményekhez\n","- Készítsünk WordCloud-ot a SMS adataira!"],"metadata":{"id":"fyfdxLEmn43z"}},{"cell_type":"code","source":["df"],"metadata":{"id":"k4UhHdmXoNXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_spam = \" \".join(df[df['label'] == 'spam'][\"message\"])\n","df_notspam = \" \".join(df[df['label'] == 'ham'][\"message\"])"],"metadata":{"id":"_-aFNPF3ntm2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# WordCloud generálása\n","plt.figure(figsize=(12, 6))\n","\n","# Spam sms\n","plt.subplot(1, 2, 1)\n","plt.imshow(WordCloud(width=800, height=400, background_color='white').generate(df_spam))\n","plt.title(\"Spam - WordCloud\")\n","plt.axis(\"off\")\n","\n","# Nem spam\n","plt.subplot(1, 2, 2)\n","plt.imshow(WordCloud(width=800, height=400, background_color='black', colormap=\"Reds\").generate(df_notspam))\n","plt.title(\"Nem Spam - WordCloud\")\n","plt.axis(\"off\")\n","\n","plt.show()"],"metadata":{"id":"B6AUiow7ntt-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Véleménydetekció (Sentiment Analysis) az SMS Spam Dataset-en\n","- Pozitív/negatív érzelmek felismerése szövegben\n","- Cél: Az SMS-ek érzelmi tónusának meghatározása NLP modellekkel\n","\n","\n"],"metadata":{"id":"thecF8KOdp_D"}},{"cell_type":"markdown","source":["Miért fontos a véleménydetekció?\n","\n","- Segít megérteni a felhasználói visszajelzéseket (pl. ügyfélszolgálati elemzések)\n","- Használható spam detektálásra (negatív hangvételű üzenetek kiszűrése)\n","- Automatikusan feldolgozza a szövegeket gépi tanulás segítségével\n","- Most megmutatjuk, hogyan végezhetünk véleményelemzést három különböző módszerrel."],"metadata":{"id":"mxjgL4j7dwy7"}},{"cell_type":"markdown","source":["### Szabályalapú véleménydetekció – VADER (NLTK)\n","-  A VADER (Valence Aware Dictionary and sEntiment Reasoner) egy szabályalapú eszköz, amely főleg rövid szövegekre, pl. tweetek és SMS-ek elemzésére alkalmas."],"metadata":{"id":"LcXjJV9gd46n"}},{"cell_type":"code","source":["from nltk.sentiment import SentimentIntensityAnalyzer\n","nltk.download(\"vader_lexicon\")\n","\n","# VADER inicializálása\n","sia = SentimentIntensityAnalyzer()\n","\n","# Vélemény pontszámok kiszámítása\n","df[\"sentiment_score\"] = df[\"message\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n","\n","# Pozitív/negatív/semleges besorolás\n","df[\"sentiment\"] = df[\"sentiment_score\"].apply(lambda x: \"positive\" if x > 0.05 else (\"negative\" if x < -0.05 else \"neutral\"))\n","\n","# Eredmények megtekintése\n","print(df[[\"message\", \"sentiment_score\", \"sentiment\"]].head())\n"],"metadata":{"id":"jDWC7GT-dbRw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- A VADER kiszámolja az érzelmi pontszámot az SMS-ekhez.\n","- Megkapjuk a kategóriát: \"positive\", \"negative\", vagy \"neutral\".\n","\n","Ez egy gyors és hatékony módszer rövid szövegekre!"],"metadata":{"id":"l_mlRw7cemDL"}},{"cell_type":"code","source":["print(df.loc[10,:]['message'])\n","print('------')\n","print(df.loc[10,:]['sentiment'])"],"metadata":{"id":"FkFqc-X-d85g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df.loc[5551,:]['message'])\n","print('------')\n","print(df.loc[5551,:]['sentiment'])\n"],"metadata":{"id":"uYf6LfR7efa7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Mélytanulás alapú véleménydetekció – BERT\n","A Hugging Face transformers könyvtárral finomhangolt mélytanulási modelleket használhatunk.\n"],"metadata":{"id":"xkainRO1i7UG"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# BERT alapú sentiment analysis modell betöltése\n","sentiment_model = pipeline(\"sentiment-analysis\")"],"metadata":{"id":"kM2FzFX5htzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Véleménydetekció egy SMS-re\n","sample_text = df[\"message\"].iloc[0]\n","result = sentiment_model(sample_text)\n","\n","print(f\"\\nSMS: {sample_text}\")\n","print(f\"Predikció: {result}\")"],"metadata":{"id":"Ps3C7Umbjkif"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mit csinálunk?\n","\n","- Előre betanított BERT modellt használunk érzelem felismerésére.\n","- Nagyon pontos mélytanulási modell, amely a legjobb eredményeket adja.\n","- A BERT modellek erőforrás-igényesek, de pontosabbak a szabályalapú és gépi tanulásos modelleknél!-"],"metadata":{"id":"8wrDX7QsjEQG"}},{"cell_type":"markdown","source":["## Feladatok"],"metadata":{"id":"iIFXol-OlsfK"}},{"cell_type":"markdown","source":["### 1. Adatok betöltése és előfeldolgozás\n","\n","- Töltsd be a Twitter vélemények adathalmazt Pandas segítségével.\n","- Ellenőrizd, hogy milyen oszlopok vannak benne és hány rekordot tartalmaz.\n","- Nézd meg az első 5 sort.\n","\n","link: https://raw.githubusercontent.com/sharmaroshan/Twitter-Sentiment-Analysis/refs/heads/master/train_tweet.csv"],"metadata":{"id":"sYBBiH2qllqK"}},{"cell_type":"code","source":[],"metadata":{"id":"FCPbstZgkucg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Szövegtisztítás és tokenizálás\n","- Tisztítsd meg a Twitter szövegeket!\n","- Távolítsd el a speciális karaktereket, URL-eket és számokat.\n","- Tokenizáld a mondatokat szavakra."],"metadata":{"id":"qGWgcsYgnH52"}},{"cell_type":"code","source":[],"metadata":{"id":"6aUons6_pL_T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Szövegek hossza, histogram\n","- Határozd meg hogy melyik szöveg milyen hosszú.\n","- Számold ki, hogy mennyi az átlagos hossza a pozitív és a negatív üzeneteknek.\n","- Készíts egy-egy histogramot a szövegek hosszának gyakoriságáról a pozitív és a negatív tweetek esetén."],"metadata":{"id":"_MTHbMzQtP0w"}},{"cell_type":"code","source":[],"metadata":{"id":"Pn5krLARuBoq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Szavak vizualizálása (WordCloud)\n","- Készíts egy WordCloud-ot a leggyakoribb pozitív és negatív szavakról."],"metadata":{"id":"Phm9QMwLpMKK"}},{"cell_type":"code","source":[],"metadata":{"id":"0nivpzGQpMZb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. BERT használata véleménydetekcióra\n","- Használj Hugging Face BERT modellt a Twitter vélemények osztályozására.\n","- Próbálj ki egy pozitív és egy negatív szöveget."],"metadata":{"id":"eLwckhvPnU0W"}},{"cell_type":"code","source":[],"metadata":{"id":"faRq7gB6nYy_"},"execution_count":null,"outputs":[]}]}